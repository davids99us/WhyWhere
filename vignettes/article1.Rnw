\documentclass{article}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{float}
\floatplacement{figure}{H}
\usepackage{lineno}
\linenumbers
%\VignetteIndexEntry{Using whywhere}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} 
\usepackage{listings}
\lstdefinestyle{myCustomMatlabStyle}{
  language=R,
  stepnumber=1,
  basicstyle=\footnotesize,  
  tabsize=4,
  showspaces=false,
  showstringspaces=false
}
\DeclareTextFontCommand{\emph}{\bfseries}



\title{A prediction algorithm for niche modeling on big environmental data}
\author{David R.B. Stockwell\footnote{All correspondence to be addressed to the author at david.r.stockwell@cqu.edu.au, Adjunct Researcher at Central Queensland University.}}
\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle
\date

\begin{abstract}
This package is an R implemention of the \pkg{WhyWhere} data mining algorithm \cite{Stockwell:2006xt}. Developed for biodiversity modelling on big data, the approach is simple and rigorous, efficient to compute, and provides accuracy equal to the best alternative approaches.  Thus, it represents the way forward in utilize large spatial environmental relationships with arbitrary response relationships.
\end{abstract}

\section{Introduction} \label{sec:intro}

This package is concerned with a methodology for the development of models of species distributions that overcomes the serious limitations to most models.  The approach could potentially be used in the other fields that require geospatial prediction.  Large 2D geospatial data arises from global coverage at fine resolution of temperature, rainfall and other surfaces that might potentially be correlated with some entity of interest.  The novel contribution of WhyWhere is to implement an algorithm that addressed the big data problem, as the size of data sets expand there is a need to reexamine traditional approaches to data analysis with expensive memory or computational demands and if necessary redesign the algorithms that in order to achieve the same purposes.  A number of features were proposed in the WhyWhere algorithm in \cite{Stockwell:2006xt} to remedy issues that often block or limit application of other algorithms in some way:

\begin{enumerate}
  \item Memory limitations: In most the \pkg{dismo} package all methods take prepared data in a 'wide' file format, i.e. similar to an excel spreadsheet.  All environmental data must be in memory for the data brick which can exceed the memory limitations of the machine.  No more than one environmental data set in memory at one time yet must still develop models composed of multiple variables.
  \item Incompatible resolutions: While For this the data is extracted from a raster data brick with the same resolution.  In the \pkg{raster} package data  coarse scale data-sets are enlarged to have the same resolution in order to form a data "brick" of coordinated grids of the same resolution.  This is approach is memory expensive.  Nice to avoid.
  \item Mixed types and distributions: Robust to distributional properties. Some methods such as generalized linear regression (GLMs) are very sensitive to distributional assumptions. Some methods done handle both continuous and categorical variables. 
\end{enumerate}

The guide is arranged as follows.  Section 2 illustrates the basic usage.  Section 3 describes the theoretical basis.  Section 4 compares the performance with the popular MaxEnt method on well known data-sets.

\section{Related Work}

The basic data flow for predicting distribution, the sampling background points, developed in \cite{Stockwell:1999yg} and commonly used for species distribution modeling and described in the package is similar to the \pkg{dismo} which implements a number of species distribution models (MaxEnt, Bioclim, Domain, GLM, GAM, and RandomForest). Species distribution models are typically built from two data sources: the coordinates of the species of interest, and the environmental values, obtained as 2D map layers.  A number of advances have been implemented since the original \pkg{WhyWhere} implementation.  We make use of the \pkg{raster} package for handling gridded spatial data \cite{Etten2012} and also the \pkg{data.table} package for fast aggregation of large data \cite{Dowle2014}. The model is developed to optimize the Area Under Curve statistic rather than a Chi-squared statistic.  

\subsection{Arbitrary Distributions} 

Most modeling methods make assumptions about th distribution.  And so they are limited.  In the original WhyWhere \cite{Stockwell:2006xt} a pcolor quantization algorithm was employed to segment the environmental space into groups in order to assign predicted probabilities for each group.  
Elements of justification of the approach are spread throughout the literature.   The basic approach, that of segmentation of the continuous variables into equal portions has been around since early image processing days.  The GIF format would compress images by reducing the number of colors in the red, green, blue 3D space.  Tests  found superiority of the median cut algorithm, for color reduction where each class has an equal number  \cite{Heckbert82}.  Here we quantiles for the same reason. 

The cuts in 3D space were used in order to handle conjunctions of variables.  This is avoided in the current algorithm as explained, with only medial cuts instead.

Median cuts have a good statistical basis as uniform sizes minimize the variability of the estimates of the posterior probability.  Experiments performed with other schemes such as equal cuts were inferior.  The exception is the factor variable which uses only those values in the layer.  When using factor variables all factors are labeled as such.

The benefit of the approach is that it does not make assumptions about the distribution.  Many make inappropriate assumptions about the distribution, even linear or monotonically increasing.  In ecological niche theory a species is more abundant, or be more likely,  within a limited range of an environmental variable.  The typical variables used are temperature  and precipitation ranges, and here a uni-modal or humped distribution is the simplest viable distribution.  In addition, many environmental variables are categorical variables such as soil and vegetation type, so the approach is capable of handling the true relationships of species to environment without making assumptions about them.

The use of presence only needs justification.  The locations where the species occurs can be thought of as a sampling of the environmental space.  That is, while the counts of the environmental values has a distribution $P_0$ for over the range of the variable, the distribution of the sample where the species is present is $P_1$ over the same values.  The most significant variable has the greatest difference between these distributions.  The membership function is developed from the ration of counts   $P_1/P_0$. 



When we look at the change in this distribution between the background counts and the counts of the species sample, the classic approach to evaluating the significance of the  the Chi-squared test, and this is what was used in the original \pkg{WhyWhere} package. The K-S test can also be used. However Chi2 doesn't work in evaluating the multivariate case.  Turns out we don't need it.  If we work out the probability for each of the training data points and apply fuzzy set methodology to these vectors the resulting vector of probabilities can be evaluated using the AUC.  

In another derivation the membership function can be viewed as a relaxation of some aspects of strict Bayesian statistics.  

\begin{equation}
P(S|E)=P(E|S)P(S)/P(E)
\end{equation}

But the problem is ratios of counts are not strictly probabilities.  In any sense, probability of the occurrence of a species $P(S)$ is not well defined, or not generally of interest.  In a typical example the probably of finding a species is dependent on season, search effort, and so is not well controlled.  Usually we only want to know the best areas to search for a species, as predicted by the most significant environmental variables.  So dropping the $P(S)$ we still have a proportional relationship which sufficient to compare alternative environmental variables.  

\begin{equation}
P(S|E)\propto P(E|S)/P(E)
\end{equation}

Other similar developments are using in MaxEnt where the membership is done using more complicated.  The results are very similar as the final comparative section shows.

\subsection{Incompatible resolutions}

Here we have that we do not need a conjunction of the model terms.  We do the prediction on the data to get probabilities, and combine these using fuzzy, then evaluate the value of the conjunction.  That way the conjunct model is evaluated without combining the environmental values of the two conjuncts.

The inferential basis is fuzzy set theory, where instead statements that are either true or false, a membership function describes a fuzzy truth value as a function $f: \mathbb{R} \rightarrow [0,1]$ from a variable $V$ to the real unit interval $[0,1]$.  One must consider membership functions taking values from other spaces such as categories, (also known as factors in R) $N$ or on a space of many variables $V_1 \times V_2 \times ... \times V_n$ where each $V_i$ is an interval in $N$ or $R$. 

Experience has shown that a particularly useful way of combining unitary membership functions to resemble the AND, OR and NOT operators of classical logic are Zadeh operators:

AND: $x \land y = min(f(x),f(y))$, \\
OR: $x \lor y = max(f(x),f(y))$, and \\
NOT: $ \lnot x = (1-f(x))$.  


There have been many approaches to learning fuzzy rules from from given data, and approaches to representation of the discovered rules.  One of the outstanding problems is the trade-off between accuracy and interpretability, or prediction and explanation in the ecological literature.  In particular in this package, ecological theory can motivate our approach, thus satisfying both criteria.  It is the desire to address the problem of explanatory models that motivated t development of \pkg{WhyWhere} -- to describe Why is a species Where? -- without sacrificing predictive accuracy or computational tractability.  

In the one dimensional case the variable with the largest significance is the best to select.  This gives a list of variables that the species responds to the strongest.  

But when we add more responses into a fuzzy expressions in a multi-variable data-sets the most accurate conjunctive expression may not contain the best single variable.  This means we cannot use such methods as greedy search to identify higher dimensional expressions.  The problem of expressing environmental relationships of more than one variable has been an important topic in statistical and ecological research, generalized linear modelling {} machine learning {} on the other.  Here we need to consider the logical relationship between variables that are expected from ecological theory.


\subsubsection{Ecological Laws}

The principle of Liebig's Law of the Minimum states that growth is controlled by the scarcest necessary resource.  This is logically an AND operation or a fuzzy conjunction of limiting factors.  Note that a model composed of an arithmetic sum would represent the concept of growth determined by the overall sum of resources contributed from different sources, and so is not consistent with the Law of the Minimum.  Most model used to predict species distributions are of this form, and so their capacity to explain species distributions is questionable. 

In ecology, Gauss's law of competitive exclusion is a proposition that two species competing for the same resource cannot coexist at constant populations if all other things remain equal, due to effect of slight advantages magnified over generations.  This behavioral shift leads to ecological niches.  This is logically an OR operation or a fuzzy disjunction:

A possible example of this case is where a set of location actually contains two different species with different different habitats.  A disjunction of two habitats may model this case better.


To evaluate both unit variate and multivariate combination of environmental variables, we predict the probability of presence on each location and calculate the area under the curve of the receiver operating statistic (AUC for short).  

There is a further statistic that is the area under rank correlation or AUC. This arises where a models outputs a figure in the range of zero to one, but a prediction must be made of 0 or 1. We must select a cutoff value for the prediction to assign to zero or one.  The AUC is the probability that would use the optimal accuracy.

A high AUC indicates that sites with high membership are more likely to be areas of presence, and vice versa.  An AUC score of 0.5 is no better than random.  The AUC can be calculated from.


\subsection{WhyWhere Algorithm}

Figure~\ref{fig2} shows a table output for the highest rated variable in the Bradypus dataset.  The categorical ranges are listed on the \emph{x} axis, the prior distribution (background or 0s) is the black solid line, and the distribution of presences (1s) in each category is the dashed line.  The posterior probability is the product of these (gray bars).  Note that the almost uniform distribution of background classes due to the quantile cuts.

The procedure in membership is as follows:

\lstset{basicstyle=\tiny,style=myCustomMatlabStyle,caption={Listing of the main algorithm},label=alg1}
\begin{lstlisting}[frame=single]
get the file in raster format
crop according to extent
extract environmental values from the file
determine number of breaks
cut into variable into quantiles (or factors)
construct table of factors with number of counts in each
calculate posterior probability
label row with probability 
calculate AUC
return vector of items labeled with probability
\end{lstlisting}

\subsubsection{One file at a time}


The inputs to \pkg{WhyWhere} are as follows: a \pkg{data.table} with the longitude and latitude of known locations, and a list of environmental data files that may be read into the raster package.  A couple of other parameters are available:   \emph{multi} for conducting the search in multiple dimensions, \emph{limit} for the minimum for entry into the results, \emph{e} the size of border around presence points, and \emph{plot} if you want to plot as you go.  The result is  a list of components:  results, lookup, etc.  The main algorithm implements a classic beam search as follows:

\lstset{basicstyle=\tiny,style=myCustomMatlabStyle,caption={Listing of the main algorithm},label=alg1}
\begin{lstlisting}[frame=single]
label input coordinates with 1 and find range
generate random points within this range and label with 0
for all environmental files do
 develop membership function for file
 bind variable name and AUC to result 
 test conjunct of this variable with best
 if result is better then bind to list
 delete last row from result if too many
\end{lstlisting}

The two analytic steps of interest are the membership function and the combination of variables into expressions.  The membership function takes the following inputs and outputs model data: \emph{file} is a single environmental file, \emph{ext} is the geographic extent, presence and absence locations, \emph{pa} a vector of zeros and ones for presence and absence locations, and returns a model object \emph{membership}.  The model consists of a table of ranges of the variable, or factors for a categorical variable, with the counts and posterior probability in each category. 




Returning to the main algorithm, calculated before calling membership: the geographic extent, and the random sample of background data.  This is necessary as the clip and background points must not vary.   If the supplied species data does not contain background data,  absences, then they are generated.  

The membership function is used to produce a vector of probabilities.  Vectors of probabilities for each variable are then combined using the fuzzy maximum principle to maximize the AUC.  The rationale for this approach and comparison is in the following section.The output is a table of the best $d$ variables as indicated by the AUC.  

This scheme allows only one environmental variable to be loaded at a time.  The rasters may have different resolutions, as the environmental values are extracted only in the one environmental variable. The variable is also cropped at this time.  We also test a combination of the variables with the next best variables by applying the minimum of the item probability vectors and recalculating the AUC.  It is possible to monitor the progress of \pkg{WhyWhere} with the plot option.  This plots out the best model sofar and prints out a list of the best models considered.

\subsection{Theoretical justifications}

\section{Experiments}

\subsection{Bradypus}

 Figure~\ref{fig1} illustrates a complete analysis on the Bradypus variegatus example file  using models  in the package \pkg{dismo} recorded in paper \cite{dismo}. Feeding brown-throated three-toed sloth (Bradypus variegatus). Consist of 9 environmental files described in \cite{dismo}.  

\begin{figure}[htbp]
\begin{center}
<<fig=T,echo=T,eval=T,png=FALSE, pdf=TRUE>>=
source("../R/ww2.R")
files <- list.files(path="/home/davids99us/data/dismo",pattern='grd', full.names=TRUE )
file <- paste(system.file(package="dismo"), '/ex/bradypus.csv',sep='')
Pres <- fread(file,  header=T,sep=",")
Pres$species=NULL
result=ww(Pres,files,plot=FALSE,multi=TRUE,trim=FALSE)
plot.ww(result)
@
\caption{\label{fig1} \pkg{WhyWhere} predicted distribution of the Bradypus data set.}
\end{center}
\end{figure}

Table~\ref{tab1} this shows the lookup table which lists the splits and the probabilities associaded with them.We test other models against the response output for the single dimensional case.  The response functions for MaxEnt and \pkg{WhyWhere} are quite similar as follows - when we can get the rgdal package installed!!!


<<echo=FALSE, results=tex>>=
xtable(result$lookup,caption="These are the results", label="tab1")
@



The figure panels show the top variable A, B a listing of the AUC of the top models, C the lookup table that is the basis of the model, and the predicted distribution with the presence points plotted.  In this distribution the result is very similar to the results from GARP and MaxEnt in paper \cite{Phillips2006231}.  Table~\ref{tab2} lists the results.

<<echo=FALSE, results=tex>>=
xtable(result$result,caption="These are the results", label="tab2")
@


 The result may be compared with the AUC from a number of other algorithms  on Table~\ref{tab2}.  Attempted to represent the protocol in \cite{dismo} which was .  Results WhyWhere 0.93 on bio16 and $fact_biome$.  The models that perform best in clude geographic models.  Theres are not in a split test protocol.  However it is possible that geographic works well when the environmental variables are no good.  So WW can add these in automatically as well.
 

<<echo=FALSE, results=tex>>=
compresults=fread("table.txt")
setorder(compresults,-AUC)
xtable(compresults,caption="These are the comparative results", label="tab3")
@


\subsection{World Data}


The above data is standard in that it is based on prepared data.  To illustrate the general purpose, have a prepared dataset of layers of 940 world extent from NCDC.  These contain many groups of variables listed in  \cite{Stockwell:2006rf} including satellite greening, monthly temperature and rainfall and many others.  They are also of different resolution from x to topo data sets with resolutions.

\begin{figure}[htbp]
\begin{center}
\includegraphics{article-005.pdf}
<<fig=T,echo=T,eval=FALSE,png=FALSE, pdf=TRUE>>=
Tfiles <- list.files(path="/home/davids99us/data/Terrestrial", 
                     pattern='pgm', full.names=TRUE )
result1=ww(Pres,Tfiles,trim=TRUE,multi=FALSE)
plot.ww(result1)
@
\caption{\label{fig1} \pkg{WhyWhere} predicted distribution of using world data set.}
\end{center}
\end{figure}

Below is listed the top results from the algorithm.  We note the accuracy is greated than the prepared data sets.  This was showin the improvement in accuracy from using this approach as noted in   \cite{Stockwell2006188}.



\section{Conclusion and Further Work}

We have found out that the WhyWhere method provides a new interface for interaction with sources of data and peoples models. The changes to methods implemented can bring to the whole system produce verifyable increases in accuracy. For example, your method can enable processing of cloud based data sets of different resulutions a source that were not possible before.  The  new method can reduce few steps in the whole work flow of data processing by separating the conjunction process. These are just two examples, there are many ways to find out if your invention is in use. Once the system has been appled to others, we can find other applications based on such points only.  The R package \pkg{WhyWhere} is a useful packages to fit, plot and test empirical species as a conjunction of response functions.  More complex logical expressions are planned, as are improvements in computational efficiency and access to cloud data.   If you cannot find such points, which is very likely for data analytics method, then keep then it i still a useful and intuitive method.  


\bibliographystyle{plain}
\bibliography{../../library}
\end{document}